---
layout: pos
title:  "Singular Value Decomposition"
image:  /images/SVD-img1.png
image1: /images/SVD-img2.png
vid: /images/SVD.mp4
toc: true


---

## Methodology

### Geometric interpretation
* The image of unit sphere under any $m \times n$ matrix is a $hyperellipse$

* Give a unit sphere $\mathbf{S}$ in $\\mathbb{R}^n$ , let $\mathbf{A} \mathbf{S}$ denote the shape after transformation

* SVD is

$$\mathbf{A}= \mathbf{U} \mathbf{\Sigma} \mathbf{V}^*$$

  where $\mathbf{U} \in \mathbb{C}^{m×m}$ and $\mathbf{V} \in \mathbb{C}^{n×n}$ is unitary and $\mathbf{\Sigma} \in \mathbb{R}^{m×n}$ is diagonal
<br/>

* Singular values are diagonal entries of $\mathbf{\Sigma}$, correspond to the principal semiaxes, with entries $\sigma_1 \ge \sigma_2 \ge \cdot \cdot \cdot \ge \sigma_n \ge 0$

* Left singular vectors of $\mathbf{A}$ are column vectors of $\mathbf{U}$ and are oriented in the directions of the principal semiaxes of $\mathbf{A} \mathbf{S}$

* Right singular vectors of $\mathbf{A}$ are column vectors of $\mathbf{V}$ and are the preimages of the principal semiaxes of $\mathbf{A} \mathbf{S}$

* $\mathbf{A} \mathbf{v}_j = \sigma_j \mathbf{u}_j$ for $1 \le j \le n$

<br/>

### Two types of SVD
**Full SVD**: $\mathbf{U} \in \mathbb{C}^{m×m}$, $\mathbf{\Sigma} \in \mathbb{R}^{m×n}$ and $\mathbf{V} \in \mathbb{C}^{n×n}$ is

$$\mathbf{A}= \mathbf{U} \mathbf{\Sigma} \mathbf{V}^*$$

**Reduced SVD**: $\hat{\mathbf{U}} \in \mathbb{C}^{m×m}$, $\hat{\mathbf{\Sigma}} \in  \mathbb{R}^{n×n}$ (assume $m \ge n$)

$$\mathbf{A}= \hat{\mathbf{U}} \hat{\mathbf{\Sigma}}  \mathbf{V}^*$$

* Furthermore, notice that

$$\mathbf{A}= \sum^{min\{m,n\}}_{i=1} \sigma_i \mathbf{u}_i \mathbf{v}_i$$

so we can keep only entries of $\mathbf{U}$ and $\mathbf{V}$ corresponding to nonzero $\sigma_i$

### Existence of SVD


**Every matrix $\mathbf{A} \in \mathbb{C}^{m×n}$ has an SVD**

<br/>

Proof: Let $\sigma = \Vert{A}\Vert_2$. There exists $\mathbf{v}_1 \in \mathbb{C}^n$ with $\Vert{v_1}\Vert_2 = 1$ and
$\Vert{Av_1}\Vert_2 = \sigma_1$ . Let $\mathbf{U}_1$ and $\mathbf{V}_1$ be unitary matrices whose first columns are
 $\mathbf{v}_1= \frac{\mathbf{A} \mathbf{v}_1}{\sigma_1}$ (or any unit-length vector if $\sigma_1 = 0$) and $\mathbf{v}_1$, respectively.
<br/>
Note that

$$\mathbf{U}_1^* \mathbf{A} \mathbf{V}_1 = \mathbf{S}= \begin{bmatrix}
    \sigma_1 & \omega^* \\
    \mathbf{0} & \mathbf{B}
\end{bmatrix}$$

<br/>

Furthermore, $\omega =0$ because $\Vert{S}\Vert_2 = \sigma_1$, and

$$\Biggl\Vert{\begin{bmatrix}
    \sigma_1 & \omega^*  \\
    \mathbf{0} & \mathbf{B}
  \end{bmatrix} \begin{bmatrix}
    \sigma_1  \\
   \omega
  \end{bmatrix}}$$
  $\ge \sigma_1^2+ \omega^* \omega = \sqrt{\sigma_1^2 + \omega^* \omega} \begin{bmatrix}
    \sigma_1  \\
   \omega
  \end{bmatrix}\Biggl\Vert_2 $

implying that $\omega_1 \ge \sqrt{\sigma_1^2 + \omega^* \omega}$ and $\omega =0$

* We then prove by induction using (1). If $m = 1$ or $n = 1$, then $\mathbf{B}$ is empty and we have $\mathbf{A} = \mathbf{U}_1 \mathbf{S}\mathbf{V}^*_1$.

* Otherwise, suppose $\mathbf{B} = \mathbf{U}_2 \mathbf{\Sigma}_2 \mathbf{V}^*_2$ , and then

$$\underbrace{\mathbf{U}_1\begin{bmatrix}
    \sigma_1 & \mathbf{0}^*  \\
    \mathbf{0} & \mathbf{\Sigma}_2
  \end{bmatrix}}_\mathbf{U}\underbrace{\begin{bmatrix}
    \sigma_1 & \mathbf{0}^*  \\
    \mathbf{0} & \mathbf{\Sigma}_2
  \end{bmatrix}}_\mathbf{\Sigma}\underbrace{\begin{bmatrix}
    1 & \mathbf{0}^*  \\
    \mathbf{0} & \mathbf{V}^*_2
  \end{bmatrix}\mathbf{V_1}^*}_\mathbf{V^*}$$

where $\mathbf{U}$ and $\mathbf{V}$ are unitary.


### Uniquesness of SVD


<blockquote title="Blockquote title">
  <p>
  Theorem:

  (Uniqueness) The singular values $\{\sigma_j \}$ are uniquely determined. If $\mathbf{A}$ is
  square and the $\sigma_j$ are distinct, the left and right singular vectors are
  uniquely determined up to complex signs (i.e., complex scalar factors of
  absolute value 1).</p>
</blockquote>



Geometric argument: If the lengths of semiaxes of a hyperellipse are
distinct, then the semiaxes themselves are determined by the geometry up
to signs.

Algebraic argument: Based on 2-norm and prove by induction. Consider
the case where the $\sigma_j$ are distinct. The 2-norm is unique, so is $\sigma_1$.If $\mathbf{v}_1$ is not unique up to sign, then the orthonormal bases of these vectors are right singular vectors of $\mathbf{A}$, implying that $\sigma_1$ is not a simple singular value.


Once $\sigma_1$ , $\mathbf{v}_1$ , and $\mathbf{v}_1$ are determined, the remainder of SVD is determined
by the space orthogonal to $\mathbf{v}_1$ . Because $\mathbf{v}_1$ is unique up to sign, the orthogonal subspace is uniquely defined. Then prove by induction

* Question: What if we change the sign of a singular vector?

* Question: What if $\sigma_i$ is not distinct?

### Matrix Properties via SVD
* Let $r$ be number of nonzero singular values of $\mathbf{A} \in \mathbb{C}^{m x n}$
  * rank($\mathbf{A}$)  is $r$
  * range($\mathbf{A}$) $=<\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_r>$
  * null($\mathbf{A}$) $=<\mathbf{u}_{r+1}, \mathbf{u}_r, ..., \mathbf{u}_r>$
<br/>
2-norm and Frobenius norm
* $\Vert{A}\Vert_2 = \sigma_1$ and $\Vert{A}\Vert_F= \sqrt{\sum_i \sigma_i^2}$

Determinant of matrix
For $\mathbf{A} \in \mathbb{C}^{m x n}$, $|det(\mathbf{A})|= \Pi_{i=1}^m \sigma_i$

* However, SVD may not be the most efficient way in solving problems


## Code

{% highlight python %}

def svd(X):
    U, Sigma, Vh = np.linalg.svd(X,
      full_matrices=False, # It's not necessary to compute the full matrix of U or V
      compute_uv=True)
{% endhighlight %}
**Reconstructing an image with Lower Rank**

{% highlight python %}

A = imread("selfie.PNG")
X=np.mean(A,-1) #averaging over 3rd dimension for 2D
img = plt.imshow(X)

img.set_cmap('gray')
plt.title("Original image")
plt.show()



def Rank(M):
    print("shape", np.shape(M))
    (U, s, Vt) = np.linalg.svd(M, full_matrices=False)
    rank = len(s)
    return print("rank",rank)

Rank(X)

def reconstruct_w_lower_Sigma(M,rank):
    (U, s, Vt) = np.linalg.svd(M, full_matrices=False)
    S = np.diag(s)
    i = 0
    for k in rank:
        Rec_pic = U[:, :k].dot(S[:k, :k]).dot(Vt[:k, :])
        i += 1
        plt.figure(i+1)
        img = plt.imshow(Rec_pic)
        img.set_cmap('gray')
        plt.title('k=' + str(k))
        plt.show()

rank=[10,20,30,35,45,70]
reconstruct_w_lower_Sigma(X,rank)

{% endhighlight %}

## Applications to Image processing

#### **Why SVD?**
Digital images require large amounts of memory, and often we would like to reduce the required memory storage
and still retain as much of the image quality as possible. We can consider using the singular value decomposition
(SVD) to manipulate these large sets of data, which will allow us to identify the components of the image which
contribute the least to overall image quality.

<div style="text-align: center;"> <img src="{{page.image | relative_url}}" height="300" width="1800">
</div>
<br/>

#### **Image Proccessing**

We compute the SVD of that matrix and remove the singular
values (from smallest to largest), converting the modified
matrices (with removed values) back into a series of images.
This process of decomposition can reduce the image storage
size without losing the quality needed to fully represent the
image.
In Figure 2 we can see that as more singular values are
included in the image matrix, the clarity of the image improves

<div style="text-align: center;"> <img src="{{page.image1 | relative_url}}" height="150" width="350">
</div>
